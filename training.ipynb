{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import tqdm as notebook_tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "# Don't Show Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from siarec.dataset import ASRDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1978848447.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [6]\u001b[0;36m\u001b[0m\n\u001b[0;31m    img =\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ASRDataset(Dataset):\n",
    "    def __init__(self, path, labels):\n",
    "        self.path = path\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def check_type_forward(self, in_types):\n",
    "        assert len(in_types) == 3\n",
    "\n",
    "        x0_type, x1_type, y_type = in_types\n",
    "        assert x0_type.size() == x1_type.shape\n",
    "        assert x1_type.size()[0] == y_type.shape[0]\n",
    "        assert x1_type.size()[0] > 0\n",
    "        assert x0_type.dim() == 2\n",
    "        assert x1_type.dim() == 2\n",
    "        assert y_type.dim() == 1\n",
    "\n",
    "    def forward(self, x0, x1, y):\n",
    "        self.check_type_forward((x0, x1, y))\n",
    "\n",
    "        # euclidian distance\n",
    "        diff = x0 - x1\n",
    "        dist_sq = torch.sum(torch.pow(diff, 2), 1)\n",
    "        dist = torch.sqrt(dist_sq)\n",
    "\n",
    "        mdist = self.margin - dist\n",
    "        dist = torch.clamp(mdist, min=0.0)\n",
    "        loss = y * dist_sq + (1 - y) * torch.pow(dist, 2)\n",
    "        loss = torch.sum(loss) / 2.0 / x0.size()[0]\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from siarec.model import SiameseNetwork\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, x0, x1, label):\n",
    "        self.size = label.shape[0]\n",
    "        self.x0 = torch.from_numpy(x0)\n",
    "        self.x1 = torch.from_numpy(x1)\n",
    "        self.label = torch.from_numpy(label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x0[index],\n",
    "                self.x1[index],\n",
    "                self.label[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "\n",
    "def create_pairs(data, digit_indices):\n",
    "    x0_data = []\n",
    "    x1_data = []\n",
    "    label = []\n",
    "\n",
    "    n = min([len(digit_indices[d]) for d in range(10)]) - 1\n",
    "    for d in range(10):\n",
    "        # make n pairs with each number\n",
    "        for i in range(n):\n",
    "            # make pairs of the same class\n",
    "            # label is 1\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
    "            # scale data to 0-1\n",
    "            # XXX this does ToTensor also\n",
    "            x0_data.append(data[z1] / 255.0)\n",
    "            x1_data.append(data[z2] / 255.0)\n",
    "            label.append(1)\n",
    "\n",
    "            # make pairs of different classes\n",
    "            # since the minimum value is 1, it is not the same class\n",
    "            # label is 0\n",
    "            inc = random.randrange(1, 10)\n",
    "            dn = (d + inc) % 10\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            # scale data to 0-1\n",
    "            # XXX this does ToTensor also\n",
    "            x0_data.append(data[z1] / 255.0)\n",
    "            x1_data.append(data[z2] / 255.0)\n",
    "            label.append(0)\n",
    "\n",
    "    x0_data = np.array(x0_data, dtype=np.float32)\n",
    "    x0_data = x0_data.reshape([-1, 1, 28, 28])\n",
    "    x1_data = np.array(x1_data, dtype=np.float32)\n",
    "    x1_data = x1_data.reshape([-1, 1, 28, 28])\n",
    "    label = np.array(label, dtype=np.int32)\n",
    "    return x0_data, x1_data, label\n",
    "\n",
    "\n",
    "def create_iterator(data, label, batchsize, shuffle=False):\n",
    "    digit_indices = [np.where(label == i)[0] for i in range(10)]\n",
    "    x0, x1, label = create_pairs(data, digit_indices)\n",
    "    ret = Dataset(x0, x1, label)\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--epoch EPOCH] [--batchsize BATCHSIZE]\n",
      "                             [--no-cuda] [--model MODEL] [--train-plot]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9021 --control=9019 --hb=9018 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"3737de67-fd65-4263-8a8e-3c6d6188138a\" --shell=9020 --transport=\"tcp\" --iopub=9022 --f=/tmp/tmp-397145pC1kowjpD63K.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--epoch', '-e', type=int, default=5,\n",
    "                    help='Number of sweeps over the dataset to train')\n",
    "parser.add_argument('--batchsize', '-b', type=int, default=128,\n",
    "                    help='Number of images in each mini-batch')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='disables CUDA training')\n",
    "parser.add_argument('--model', '-m', default='',\n",
    "                    help='Give a model to test')\n",
    "parser.add_argument('--train-plot', action='store_true', default=False,\n",
    "                    help='Plot train loss')\n",
    "args = parser.parse_args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "print(\"Args: %s\" % args)\n",
    "\n",
    "# create pair dataset iterator\n",
    "train = dsets.MNIST(\n",
    "    root='../data/',\n",
    "    train=True,\n",
    "    # transform=transforms.Compose([\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize((0.1307,), (0.3081,))\n",
    "    # ]),\n",
    "    download=True\n",
    ")\n",
    "test = dsets.MNIST(\n",
    "    root='../data/',\n",
    "    train=False,\n",
    "    # XXX ToTensor scale to 0-1\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    #     transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_iter = create_iterator(\n",
    "    train.train_data.numpy(),\n",
    "    train.train_labels.numpy(),\n",
    "    args.batchsize)\n",
    "\n",
    "# model\n",
    "model = SiameseNetwork()\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "# Loss and Optimizer\n",
    "criterion = ContrastiveLoss()\n",
    "# optimizer = torch.optim.Adam(\n",
    "#     [p for p in model.parameters() if p.requires_grad],\n",
    "#     lr=learning_rate\n",
    "# )\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                            momentum=momentum)\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_iter,\n",
    "    batch_size=args.batchsize, shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test,\n",
    "    batch_size=args.batchsize, shuffle=True, **kwargs)\n",
    "\n",
    "def train(epoch):\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "    start_epoch = time.time()\n",
    "    for batch_idx, (x0, x1, labels) in enumerate(train_loader):\n",
    "        labels = labels.float()\n",
    "        if args.cuda:\n",
    "            x0, x1, labels = x0.cuda(), x1.cuda(), labels.cuda()\n",
    "        x0, x1, labels = Variable(x0), Variable(x1), Variable(labels)\n",
    "        output1, output2 = model(x0, x1)\n",
    "        loss = criterion(output1, output2, labels)\n",
    "        train_loss.append(loss.data[0])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        accuracy = []\n",
    "\n",
    "        for idx, logit in enumerate([output1, output2]):\n",
    "            corrects = (torch.max(logit, 1)[1].data == labels.long().data).sum()\n",
    "            accu = float(corrects) / float(labels.size()[0])\n",
    "            accuracy.append(accu)\n",
    "\n",
    "        if batch_idx % args.batchsize == 0:\n",
    "            end = time.time()\n",
    "            took = end - start\n",
    "            for idx, accu in enumerate(accuracy):\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss:{:.6f}\\tTook: {:.2f}\\tOut: {}\\tAccu: {:.2f}'.format(\n",
    "                    epoch, batch_idx * len(labels), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.data[0],\n",
    "                    took, idx, accu * 100.))\n",
    "            start = time.time()\n",
    "    torch.save(model.state_dict(), './model-epoch-%s.pth' % epoch)\n",
    "    end = time.time()\n",
    "    took = end - start_epoch\n",
    "    print('Train epoch: {} \\tTook:{:.2f}'.format(epoch, took))\n",
    "    return train_loss\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    all = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch_idx, (x, labels) in enumerate(test_loader):\n",
    "        if args.cuda:\n",
    "            x, labels = x.cuda(), labels.cuda()\n",
    "        x, labels = Variable(x, volatile=True), Variable(labels)\n",
    "        output = model.forward_once(x)\n",
    "        all.extend(output.data.cpu().numpy().tolist())\n",
    "        all_labels.extend(labels.data.cpu().numpy().tolist())\n",
    "\n",
    "    numpy_all = np.array(all)\n",
    "    numpy_labels = np.array(all_labels)\n",
    "    return numpy_all, numpy_labels\n",
    "\n",
    "def plot_mnist(numpy_all, numpy_labels):\n",
    "    c = ['#ff0000', '#ffff00', '#00ff00', '#00ffff', '#0000ff',\n",
    "            '#ff00ff', '#990000', '#999900', '#009900', '#009999']\n",
    "\n",
    "    for i in range(10):\n",
    "        f = numpy_all[np.where(numpy_labels == i)]\n",
    "        plt.plot(f[:, 0], f[:, 1], '.', c=c[i])\n",
    "    plt.legend(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "    plt.savefig('result.png')\n",
    "\n",
    "if len(args.model) == 0:\n",
    "    train_loss = []\n",
    "    for epoch in range(1, args.epoch + 1):\n",
    "        train_loss.extend(train(epoch))\n",
    "\n",
    "    if args.train_plot:\n",
    "        plt.gca().cla()\n",
    "        plt.plot(train_loss, label=\"train loss\")\n",
    "        plt.legend()\n",
    "        plt.draw()\n",
    "        plt.savefig('train_loss.png')\n",
    "        plt.gca().clear()\n",
    "\n",
    "else:\n",
    "    saved_model = torch.load(args.model)\n",
    "    model = SiameseNetwork()\n",
    "    model.load_state_dict(saved_model)\n",
    "    if args.cuda:\n",
    "        model.cuda()\n",
    "\n",
    "numpy_all, numpy_labels = test(model)\n",
    "plot_mnist(numpy_all, numpy_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2430eaf48f5463d797560f7f0437af2b561f61bd8cf8286414d2ff27c781618"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.asr': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
